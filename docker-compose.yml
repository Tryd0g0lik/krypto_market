# If you want to look the log files,
# Mean , run the command
# >> docker inspect --format='{{.LogPath}}' <container_name_or_id> (in your  console/cmd )
services:
    # =====================
    # PostgreSQL Database
    # =====================
    db:
        container_name: ${COMPOSE_PROJECT_NAME:-fastapi}_postgres_data
        image: postgres
        env_file: ./.env
        environment:
            - POSTGRES_USER=${POSTGRES_USER}
            - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
            - POSTGRES_DB=${POSTGRES_DB}
        healthcheck:
            test: [ 'CMD-SHELL', 'pg_isready -d ${POSTGRES_DB} -U ${POSTGRES_USER}' ]
            interval: 5s
            timeout: 5s
            retries: 5
        restart: unless-stopped
        ports:
            - "5432:5432"
        networks:
            backend:
                aliases:
                    - db
                    - postgres
        logging:
            driver: "json-file"
            options:
                max-size: "10m"
                max-file: "3"
    # =====================
    # FastAPI Application
    # =====================
    api:
        container_name: ${COMPOSE_PROJECT_NAME:-fastapi}_static
        healthcheck:
            test: [ "CMD", "curl", "-f", "http://0.0.0.0:8003/health" ]
            interval: 30s
            timeout: 10s
            retries: 3
        build:
            dockerfile: ./Dockerfile
            context: .
        env_file: ./.env
        environment:
            POSTGRES_USER: ${POSTGRES_USER}
            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
            POSTGRES_DB: ${POSTGRES_DB}
            DATABASE_HOST: ${POSTGRES_HOST}
            DATABASE_PORT: ${POSTGRES_PORT}
            REDIS_HOST: 'redis'
            REDIS_PORT: ${REDIS_PORT}
            PYTHONPATH: /www/src
        volumes:
            - ./:/www/src
            - volume_static:/www/src/collectstatic
            - volume_media:/www/src/media
        command:
            - bash
            - -c
            - |
                set -e
                echo "Waiting for database..."

                MAX_RETRIES=30
                RETRY_COUNT=0

                until python -c "
                import psycopg2
                import os
                import sys
                try:
                    conn = psycopg2.connect(
                    host=os.environ['DATABASE_HOST'],
                    user=os.environ['POSTGRES_USER'],
                    password=os.environ.get('POSTGRES_PASSWORD'),
                    dbname=os.environ.get('POSTGRES_DB')
                    )
                    conn.close()
                    sys.exit(0)
                except Exception as e:
                    print(f'Connection failed: { e }')
                    sys.exit(1)
                "; do
                            RETRY_COUNT=$((RETRY_COUNT + 1))
                            if [ $RETRY_COUNT -ge $MAX_RETRIES ]; then
                                echo "Docker-file: Failed to connect to PostgreSQL after $MAX_RETRIES attempts"
                                exit 1
                            fi
                            echo "Docker-file: Waiting for PostgreSQL... attempt $RETRY_COUNT/$MAX_RETRIES"
                            sleep 2
                        done
        
                        echo "Docker-file: PostgreSQL is ready!"
                        echo "Creating schema 'crypto' if not exists..."
                                                   
                python -c "
                import psycopg2
                import os
                conn = psycopg2.connect(
                host=os.environ['DATABASE_HOST'],
                user=os.environ.get('POSTGRES_USER'),
                password=os.environ.get('POSTGRES_PASSWORD'),
                dbname=os.environ.get('POSTGRES_DB')
                )
                conn.autocommit = True
                cur = conn.cursor()
                cur.execute('CREATE SCHEMA IF NOT EXISTS crypto')
                cur.close()
                conn.close()
                print('Docker-file: Schema crypto created')
                "
                
                echo "Running migrations..."
                alembic downgrade base
                alembic upgrade head
                echo "Starting application..."
                python -m cryptomarket.main

#                alembic downgrade base
#                rm alembic/versions/*.py
#                alembic revision --autogenerate
#                alembic upgrade head
#                python -m cryptomarket.main
        ports:
            - "8003:8003"
        depends_on:
            db:
                condition: service_healthy
            redis:
                condition: service_healthy
        restart: unless-stopped
        networks:
            backend:
                aliases:
                  - api
                  - backend
        logging:
            driver: "json-file"
            options:
                max-size: "10m"
                max-file: "3"
    # =====================
    # Celery Worker
    # =====================
    celery_worker:
        container_name: ${COMPOSE_PROJECT_NAME:-fastapi}_celery_worker
        build:
            context: .
            dockerfile: ./Dockerfile  # Тот же Dockerfile, что и для backend
        env_file: ./.env
        environment:
            CELERY_BROKER_URL: redis://:${REDIS_PASSWORD:-}@redis:${REDIS_PORT}/0
            CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD:-}@redis:${REDIS_PORT}/0
        command:
            - sh
            - -c
            - "celery -A cryptomarket.project.celery_ worker --loglevel=info"

        volumes:
            - .:/www/src
        depends_on:
            db:
                condition: service_healthy
            redis:
                condition: service_healthy
            api:
                condition: service_healthy
        restart: unless-stopped
        networks:
            - backend
        logging:
            driver: "json-file"
            options:
                max-size: "10m"
                max-file: "3"
    # =====================
    # Celery Beat (Scheduler)
    # =====================
    celery_beat:
        container_name: ${COMPOSE_PROJECT_NAME:-fastapi}_celery_beat
        build:
            context: .
            dockerfile: ./Dockerfile
        env_file: ./.env
        environment:
            CELERY_BROKER_URL: redis://:${REDIS_PASSWORD:-}@redis:${REDIS_PORT}/0
            CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD:-}@redis:${REDIS_PORT}/0
        command:
            - sh
            - -c
            - "celery -A cryptomarket.project.celery_ beat --loglevel=info"
        volumes:
            - .:/www/src
        depends_on:
            db:
                condition: service_healthy
            redis:
                condition: service_healthy
            celery_worker:
                condition: service_started
        networks:
            - backend
        restart: unless-stopped
    # =====================
    # Flower (Celery Monitoring)
    # =====================
    flower:
        container_name: ${COMPOSE_PROJECT_NAME:-fastapi}_flower
        image: mher/flower
        env_file: .env
        environment:
            CELERY_BROKER_URL: redis://:${REDIS_PASSWORD:-}@redis:${REDIS_PORT}/0
            CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD:-}@redis:${REDIS_PORT}/0
            FLOWER_PORT: 5555
            FLOWER_BASIC_AUTH: ${FLOWER_USER:-admin}:${FLOWER_PASSWORD:-admin}
        ports:
            - "${FLOWER_PORT:-5555}:5555"
        depends_on:
            redis:
                condition: service_healthy
            celery_worker:
                condition: service_started
        restart: unless-stopped
        networks:
            - backend
        logging:
            driver: "json-file"
            options:
                max-size: "10m"
                max-file: "3"
    # =====================
    # Nginx (Reverse Proxy)
    # =====================
    nginx:
        container_name: ${COMPOSE_PROJECT_NAME:-fastapi}_nginx
        build:
            context: ./nginx
            dockerfile: Dockerfile
        depends_on:
            api:
                condition: service_healthy
            db:
                condition: service_healthy
        volumes:
            - volume_static:/www/src/collectstatic
            - volume_media:/www/src/media
        restart: on-failure
        ports:
            - "80:80"
        networks:
            backend:
                aliases:
                    - nginx
                    - proxiserver
    adminer:
        container_name: ${COMPOSE_PROJECT_NAME:-fastapi}_adminer
        image: adminer:4.8.1
        ports:
            - "${ADMINER_PORT:-8080}:8080"
        depends_on:
            db:
                condition: service_healthy
        restart: unless-stopped
        networks:
            - backend
        logging:
            driver: "json-file"
            options:
                max-size: "10m"
                max-file: "3"
    # =====================
    # Redis Cache
    # =====================
    redis:
        container_name: ${COMPOSE_PROJECT_NAME:-fastapi}_redis_data
        image: redis:7.2-alpine
        healthcheck:
            test: ["CMD", "redis-cli", "ping"]
            interval: 30s
            timeout: 10s
            retries: 3

        command: redis-server --bind 0.0.0.0
        volumes:
            - redis_data:/data
        ports:
            - "6380:6380"
        networks:
            backend:
                aliases:
                    - radis
                    - cache
volumes:
    redis_data:
    volume_static:
    volume_media:

networks:
  backend:
    driver: bridge
